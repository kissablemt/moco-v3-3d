{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wzt/src/moco-v3-3d\n"
     ]
    }
   ],
   "source": [
    "%cd /home/wzt/src/moco-v3-3d/\n",
    "import vits_3d\n",
    "import torch\n",
    "import transforms as my_transforms\n",
    "import datasets as my_datasets\n",
    "import monai\n",
    "import moco\n",
    "import moco.builder\n",
    "from functools import partial\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "x = torch.load(\"x.pth\")\n",
    "checkpoint = torch.load(\"/home/wzt/src/moco-v3-3d/checkpoint_0099.pth.tar\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadstate(model, state_dict):\n",
    "    state_dict = copy.deepcopy(state_dict)\n",
    "    linear_keyword = 'head'\n",
    "\n",
    "    # freeze all layers but the last fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['%s.weight' % linear_keyword, '%s.bias' % linear_keyword]:\n",
    "            param.requires_grad = False\n",
    "    # init the fc layer\n",
    "    getattr(model, linear_keyword).weight.data.normal_(mean=0.0, std=0.01)\n",
    "    getattr(model, linear_keyword).bias.data.zero_()\n",
    "\n",
    "    # rename moco pre-trained keys\n",
    "    for k in list(state_dict.keys()):\n",
    "        # retain only base_encoder up to before the embedding layer\n",
    "        if k.startswith('module.base_encoder') and not k.startswith('module.base_encoder.%s' % linear_keyword):\n",
    "            # remove prefix\n",
    "            state_dict[k[len(\"module.base_encoder.\"):]] = state_dict[k]\n",
    "        # delete renamed or unused k\n",
    "        del state_dict[k]\n",
    "\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    assert set(msg.missing_keys) == {\"%s.weight\" % linear_keyword, \"%s.bias\" % linear_keyword}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vits_3d.__dict__[\"vit_3d_conv_small\"](img_size=48, in_chans=1)\n",
    "model = loadstate(model, checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = moco.builder.MoCo_ViT(\n",
    "    partial(vits_3d.__dict__[\"vit_3d_conv_small\"], img_size=48, in_chans=1),\n",
    "    256, 4096, 1.0).base_encoder\n",
    "model = loadstate(model, checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if k.startswith(\"module.base_encoder.patch_embed.proj.1.\"):\n",
    "        print(k)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbn = torch.load(\"nan_bn3d.pth\")\n",
    "x = torch.randn(4, 48, 24, 24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbn.train()\n",
    "nbn(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbn.eval()\n",
    "nbn.running_mean = torch.zeros_like(nbn.running_mean)\n",
    "nbn.running_var = torch.ones_like(nbn.running_var)\n",
    "nbn(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbn.eval()\n",
    "torch.isnan(nbn(torch.randn(4, 48, 24, 24, 24))).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.BatchNorm3d(48).running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = torch.nn.BatchNorm3d(48)\n",
    "bn.weight = nbn.weight\n",
    "bn.eval()\n",
    "\n",
    "torch.isnan(bn(torch.randn(4, 48, 24, 24, 24))).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset len:  39939\n"
     ]
    }
   ],
   "source": [
    "def make_transform(patch_size):\n",
    "    aug_block = my_transforms.aug_transform(size=patch_size)\n",
    "    return monai.transforms.Compose([\n",
    "        monai.transforms.EnsureChannelFirst(channel_dim='no_channel'),\n",
    "        my_transforms.Duplicate(transforms1=aug_block, transforms2=aug_block),\n",
    "    ])\n",
    "    \n",
    "traindir = '/media/wzt/plum14t/wzt/ProcressedData/ROI_resize48/images_cut_48_resize/'\n",
    "train_dataset = my_datasets.SSLNoCropDataset(traindir, size=48, transform=make_transform(48), enable_negatives=False)\n",
    "print(\"train_dataset len: \", len(train_dataset))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=False,\n",
    "    num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b64089e8fe444794cf9b1fbafbba30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for images, _ in tqdm(train_dataset):\n",
    "    x1 = images[0]\n",
    "    print(type(x1))\n",
    "    x1 = monai.transforms.ToTensor(track_meta=False)(x1)\n",
    "    print(type(x1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in tqdm(Path(\"/media/wzt/plum14t/wzt/ProcressedData/ROI_resize48/images_cut_48_resize/\").glob(\"*.nii.gz\")):\n",
    "    name = img_path.name.lower()\n",
    "    if \"c0\" in name or \"c2\" in name:\n",
    "        sym_path = Path(\"/media/wzt/plum14t/wzt/ProcressedData/ROI_resize48/train/\") / img_path.name\n",
    "        sym_path.symlink_to(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "i = 0\n",
    "for images, _ in tqdm(train_dataset):\n",
    "    row = train_dataset.get_row(i)\n",
    "    try:\n",
    "        row.update(dict(min1=images[0].min().item(), max1=images[0].max().item(), min2=images[1].min().item(), max2=images[1].max().item(), mean1=images[0].mean().item(), mean2=images[1].mean().item(), std1=images[0].std().item(), std2=images[1].std().item()))\n",
    "    except Exception as e:\n",
    "        print(i, e)\n",
    "    data.append(row)\n",
    "    i += 1\n",
    "pd.DataFrame(data).to_csv(\"data3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = moco.builder.MoCo_ViT(\n",
    "    partial(vits_3d.__dict__[\"vit_3d_conv_small\"], img_size=48, in_chans=1),\n",
    "    256, 4096, 1.0).cuda()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.6, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform(patch_size):\n",
    "    aug_block = my_transforms.aug_transform(size=patch_size, prob=1)\n",
    "    return monai.transforms.Compose([\n",
    "        monai.transforms.EnsureChannelFirst(channel_dim='no_channel'),\n",
    "        my_transforms.Duplicate(transforms1=aug_block, transforms2=aug_block),\n",
    "    ])\n",
    "    \n",
    "traindir = '/mnt/tmp/nan/'\n",
    "train_dataset = my_datasets.SSLNoCropDataset(traindir, size=48, transform=make_transform(48), enable_negatives=False)\n",
    "print(\"train_dataset len: \", len(train_dataset))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=5, shuffle=False,\n",
    "    num_workers=0, pin_memory=True)\n",
    "\n",
    "for e in tqdm(range(1000)):\n",
    "    # for images, _ in train_loader:\n",
    "        x1 = images[0].cuda()\n",
    "        x2 = images[1].cuda()\n",
    "\n",
    "        loss = model(x1=torch.zeros_like(x1), x2=torch.zeros_like(x1), m=0.99)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_encoder.patch_embed.proj[1].state_dict()[\"running_mean\"], model.base_encoder.patch_embed.proj[1].state_dict()[\"running_var\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
